name: Data-Copy | Lower-Env | RESTORE-TABLES-DATA

on:
  workflow_dispatch:
    inputs:
      Environment:
        description: 'Select an environment to restore data dump'
        required: true
        type: choice
        options: ['beta-cc', 'beta-me', 'beta-nl', 'beta-pro', 'beta-pw', 'beta-bid', 'beta-red']
      SOURCE_S3_BUCKET:
        description: 'Select S3 bucket source to restore data dump in selected environment'
        required: true
        type: choice
        options: ['dbz-data-copy-test','dbz-data-copy-prod']

  workflow_call:
    inputs:
      Environment:
        description: 'Select an environment to restore data dump'
        required: true
        type: string
      SOURCE_S3_BUCKET:
        description: 'Select S3 bucket source to restore data dump in selected environment'
        required: true
        type: string

env:
    MYSQL_CLIENT_POD_NAME_PREFIX: "data-copy-mysql-client"
    POSTGRESQL_CLIENT_POD_NAME_PREFIX: "data-copy-postgresql-client"
    POD_WAIT_TIME_IN_SECONDS: 10

jobs:

  restore-tables-data-job:
    runs-on: [self-hosted, dbz-runner-amd-v2-small]
    steps:

    - name: Clean Container
      uses: dbz/actions-clean@v2

    - name: Checkout Repository
      uses: actions/checkout@v4
      with:
        clean: true

    - name: Intialize Env
      run: |
        if [[ ${{inputs.Environment}} == beta-* ]]; then
          echo "K8S_CONTEXT=arn:aws:eks:eu-west-1:857520607940:cluster/beta-eks-cluster" >> $GITHUB_ENV
          echo "ROLE_TO_ASSUME=arn:aws:iam::857520607940:role/CrossAccount_EKS_for_MENAOps" >> $GITHUB_ENV
          SOURCE_S3_BUCKET="${{inputs.SOURCE_S3_BUCKET}}"
          echo "SOURCE_S3_BUCKET=${SOURCE_S3_BUCKET}" >> $GITHUB_ENV
        else
            echo "Wrong/Invalid Environment"
            exit 1
        fi

    - name: Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-skip-session-tagging: true
        aws-region: eu-west-1
        role-to-assume: "${{env.ROLE_TO_ASSUME}}"
        role-duration-seconds: 3600

    - name: Install Kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'v1.28.0'

    - name: Configure Kubectl
      uses: azure/k8s-set-context@v3
      with:
        method: kubeconfig
        kubeconfig: ${{ secrets.KUBE_CONFIG }}
        context: ${{ env.K8S_CONTEXT }}

    - name: Restore Tables
      run: |

        get_services_array() {
            local services=()
            local skip_headers=1
            while IFS=, read -r TARGETED_SERVICES || [ -n "$TARGETED_SERVICES" ]
            do
                if ((skip_headers))
                then
                    ((skip_headers--))
                else
                    services+=("$TARGETED_SERVICES")
                fi
            done < ./data_copy_inputfiles/dbs/targeted_services.csv
            echo "${services[@]}"
            }

        get_tables_array() {
            local service=$1
            local tables=()
            local skip_headers=1
            while IFS=, read -r TARGETED_TABLES || [ -n "$TARGETED_TABLES" ]
            do
                if ((skip_headers))
                then
                    ((skip_headers--))
                else
                    tables+=("$TARGETED_TABLES")
                fi
            done < ./data_copy_inputfiles/tables/${service}_tables.csv
            echo "${tables[@]}"
            }

        IFS=' '
        services_array=$(get_services_array)
        
        read -r -a services <<< "$services_array"
        
        # Check current context!
        kubectl config get-contexts | grep "*"

        make_temp_file() {
            local temp_dir="temp"
            local temp_file="${temp_dir}/script_temp_$(date '+%Y%m%d%H%M%S')_${RANDOM}"
            mkdir -p ./"$temp_dir"
            if [ -e "$temp_file" ]; then
                temp_file="${temp_dir}/script_temp_$(date '+%Y%m%d%H%M%S')_${RANDOM}_$$"
            fi
            touch ./"$temp_file"
            echo "$temp_file"
            }

        BEFORE_IMPORT_MYSQL_DB="SET FOREIGN_KEY_CHECKS=0;"
        AFTER_IMPORT_MYSQL_DB="SET FOREIGN_KEY_CHECKS=1;"

        BEFORE_IMPORT_POSTGRESQL_DB="SET session_replication_role = 'replica';"
        AFTER_IMPORT_POSTGRESQL_DB="SET session_replication_role = 'origin';"

        ROOT_BACKUP_DIR=root_backup/
        mkdir -p ./${ROOT_BACKUP_DIR}
        aws s3 cp s3://$SOURCE_S3_BUCKET ./${ROOT_BACKUP_DIR} --recursive --exclude "backup/*"

        for service in "${services[@]}"; do
            echo "Service Name is $service"
            
            # Get data-copy env specific secrets
            rds_secret_result=$(aws secretsmanager get-secret-value --secret-id data-copy-secrets/${{inputs.Environment}}/${service} | jq -r '.SecretString| fromjson')
            rds_host=$(echo $rds_secret_result | jq -r '.DATABASE_HOST')
            rds_username=$(echo $rds_secret_result | jq -r '.DATABASE_USER')
            rds_password=$(echo $rds_secret_result | jq -r '.DATABASE_PASSWORD')
            rds_db=$(echo $rds_secret_result | jq -r '.DATABASE_NAME')
            rds_engine=$(echo $rds_secret_result | jq -r '.DATABASE_ENGINE')

            MYSQL_CLIENT_POD_NAME=${MYSQL_CLIENT_POD_NAME_PREFIX}-${{inputs.Environment}}
            POSTGRESQL_CLIENT_POD_NAME=${POSTGRESQL_CLIENT_POD_NAME_PREFIX}-${{inputs.Environment}}

            BACKUP_DIR=root_backup/backup-and-restore-tables-${service}/
            mkdir -p ./${BACKUP_DIR}

            if [[ "$rds_engine" = "mysql" ]]; then

                # Create MySQL Client Pod
                pod_exist=$(kubectl get pods -n infra | grep $MYSQL_CLIENT_POD_NAME | { grep -v grep || true; } )
                if [[ -z "$pod_exist" ]]; then
                    kubectl run -n infra $MYSQL_CLIENT_POD_NAME --image=mysql --restart=Never --env=MYSQL_ALLOW_EMPTY_PASSWORD=true
                    sleep $POD_WAIT_TIME_IN_SECONDS
                fi

                tables_array=$(get_tables_array "$service")
                IFS=' '
                
                read -r -a tables <<< "$tables_array"

                for table in "${tables[@]}"; do

                    # Find the corresponding dump file
                    # Make sure to change file format if dump is in different format
                    DUMP_FILE=$(ls -1 "${BACKUP_DIR}${rds_db}-${table}-"*.sql 2>/dev/null | head -n 1)
                    if [[ -z "$DUMP_FILE" ]]; then
                        echo "No dump file found for table '$table'."
                        continue
                    fi

                    # Create a temporary file to store the concatenated content
                    TMP_FILE=$(make_temp_file)
                    # echo "File created is: ${TMP_FILE}"

                    # Concatenate content and store it in the temporary file
                    echo "$BEFORE_IMPORT_MYSQL_DB" > ./"${TMP_FILE}"
                    cat "$DUMP_FILE" >> ./"${TMP_FILE}"
                    echo "$AFTER_IMPORT_MYSQL_DB" >> ./"${TMP_FILE}"

                    # Restore the table from the dump file
                    echo "Restoring Table '${table}' from dump file '$DUMP_FILE' for Service '${service}'..."
                    echo "kubectl exec -n infra -it $MYSQL_CLIENT_POD_NAME -- mysql --host=$rds_host --user=$rds_username --password=xxxxxxxxxxx ${rds_db} --verbose < ./${TMP_FILE}"
                    kubectl exec -n infra -it $MYSQL_CLIENT_POD_NAME -- mysql --host=$rds_host --user=$rds_username --password=$rds_password ${rds_db} --verbose < ./${TMP_FILE}

                    # Remove the temporary file
                    rm -f ./"${TMP_FILE}"
                done

            elif [[ "$rds_engine" = "postgresql" ]]; then
                
                # Create PostgreSQL Client Pod
                pod_exist=$(kubectl get pods -n infra | grep $POSTGRESQL_CLIENT_POD_NAME | { grep -v grep || true; } )
                if [[ -z "$pod_exist" ]]; then
                    kubectl run -n infra $POSTGRESQL_CLIENT_POD_NAME --image=postgres --restart=Never --env=POSTGRES_PASSWORD=mysecretpassword
                    sleep $POD_WAIT_TIME_IN_SECONDS
                fi

                tables_array=$(get_tables_array "$service")
                IFS=' '
                
                read -r -a tables <<< "$tables_array"

                for table in "${tables[@]}"; do

                    # Find the corresponding dump file
                    # Make sure to change file format if dump is in different format
                    DUMP_FILE=$(ls -1 "${BACKUP_DIR}${rds_db}-${table}-"*.sql 2>/dev/null | head -n 1)
                    if [[ -z "$DUMP_FILE" ]]; then
                        echo "No dump file found for table '$table'."
                        continue
                    fi

                    # Create a temporary file to store the concatenated content
                    TMP_FILE=$(make_temp_file)
                    # echo "File created is: ${TMP_FILE}"

                    # Concatenate content and store it in the temporary file
                    echo "$BEFORE_IMPORT_POSTGRESQL_DB" > ./"${TMP_FILE}"
                    cat "$DUMP_FILE" >> ./"${TMP_FILE}"
                    echo "$AFTER_IMPORT_POSTGRESQL_DB" >> ./"${TMP_FILE}"

                    # Restore the table from the dump file
                    echo "Restoring Table '${table}' from dump file '$DUMP_FILE' for Service '${service}'..."
                    kubectl exec -n infra -it $POSTGRESQL_CLIENT_POD_NAME -- mkdir -p ./temp
                    kubectl cp -n infra ./${TMP_FILE} $POSTGRESQL_CLIENT_POD_NAME:${TMP_FILE}
                    echo "kubectl exec -n infra -it $POSTGRESQL_CLIENT_POD_NAME -- psql -h $rds_host -U $rds_username -W -d ${rds_db} -b -v VERBOSITY=verbose -f ./${TMP_FILE} <<< \"xxxxxxxxxxx\" "
                    kubectl exec -n infra -it $POSTGRESQL_CLIENT_POD_NAME -- psql -h $rds_host -U $rds_username -W -d ${rds_db} -b -v VERBOSITY=verbose -f ./${TMP_FILE} <<< "$rds_password"
                    
                    # Remove the temporary file
                    rm -f ./"${TMP_FILE}"
                done

            fi

        done